# python爬虫

### 1.测试Chrome的headless模式遇到警告

```python
from selenium import webdriver

opt = webdriver.ChromeOptions()
opt.set_headless()
driver = webdriver.Chrome(options=opt)
driver.get('https://www.baidu.com/')
print(driver.current_url)
```

警告如下

```python
 DeprecationWarning: use setter for headless property instead of set_headless
  opt.set_headless()
```

修改如下没有报错和警告

```python
from selenium import webdriver

opt = webdriver.ChromeOptions()
opt.headless = True
driver = webdriver.Chrome(options=opt)
driver.get('https://www.baidu.com/')
print(driver.current_url)
```

正确返回

```
https://www.baidu.com/
```

### 2.需要安装的库

##### 请求库安装

requests

Selenium

ChromeDriver

aiohttp:异步web服务库

##### 解析库安装

lxml：是python的一个解析库，支持HTML和XML的解析，支持XPath解析方式，解析效率高。

Beautiful Soup的HTML和XML解析器是依赖于lxml库的。

pyquery：提供了和jQuery类似的语法来解析HTML文档，支持CSS选择器。

tesserocr:在爬虫过程中难免遇到各种各样的验证码，而且大多数是图片验证码，这时候可以直接用OCR来识别。tesserocr是python的一个OCR识别库，但其实是对tesseract做了一层python API封装，所以核心是tesseract.因此在安装tesserocr之前，需要先安装tesseract。

下载地址：http://digi.bib.uni-mannheim.de/tesseract

我下载的是3.0稳定版本中的3.05版本。

安装完成后添加到环境变量中后进行测试，直接输入tesseract是正常的，证明安装和添加环境变量没有问题。

然后测试下面的命令

```
tesseract --list-langs
```

会报错，提示的大概意思是还需要把tessdata添加打环境变量TESSDATA_PREFIX中，所以这个需要注意下，在系统环境变量中新建一个环境变量

```
变量名：TESSDATA_PREFIX
变量值：tessdata的目录，要包含tessdata，我的是
D:\tesseract\Tesseract-OCR\tessdata
```

这样就可以在任何目录下运行上面命令了。

然后需要安装tesserocr 和 pillow库。

测试1：通过一张图片进行验证是否都安装成功以及能否正常识别图片上的文字。

```
tesseract image.png result -leng && cat result.txt
```

cat命令在windows下会报错，但是也能正常运行出结果来。替代命令时type，但是并没有cat命令好用。而且运行了一下发现没啥效果。替代方法就是不用cmd和powershell，一般都会下载git，git带了一个bash命令行工具，这个和linux命令是一样的。

测试2：用python脚本做一下测试

```python
import tesserocr

from PIL import Image

image = Image.open('image.png')
print(tesserocr.image_to_text(image))
```

报错

```
RuntimeError: Failed to init API, possibly an invalid tessdata path: D:\GitHub\spider\spider-study\.venv\Scrpits\/tessdata/
```

这个问题和上面的问题差不多，把tesseract目录里的tessdata目录复制到Scripts，也就是虚拟环境的python.exe目录下就可以了。

另外想起来了，安装tesserocr时报错了，我就直接到网上下载安装包，然后再用安装包安装，发现还是不行，仔细看下就是安装包支持的是python3.7版本，我的虚拟环境使用的是python3.8版本，无奈啊，只能把虚拟环境和之前安装的库都删光。重新用pipenv创建一个python3.7的虚拟环境。

```
pipenv install  #这个是创建一个默认python版本的虚拟环境
pipenv --python 3.7 #这个是创建一个python3.7版本的虚拟环境，之前一直没用过，还是刚学会用的。
pipenv -h #查看帮助信息，自己去网上到处查还不如直接看帮助信息
```

创建完3.7版本的再用刚才下的安装包就安装成功了。我想即使不用这个安装包，直接用pipenv install tesserocr也会安装成功的。

##### 存储库安装

pymysql

pymongo

redis

RedisDump：是一个用于redis数据导入/导出的工具，是基于Ruby实现的，所以需要先安装ruby。

安装完成后通过ruby的命令安装

```
gem install redis-dump
```

安装成功后验证如下两个命令

redis-dump

redis-load

##### web库安装

flask

tornado

##### App爬取相关库的安装

###### charles：是一个网络抓包工具，相比Fiddler，其功能更为强大。这里选用它来作为主要的移动端抓包工具。

下载地址：直接官网即可 http://www.charlesproxy.com

由于收费，破解方法

blog介绍: https://blog.zzzmode.com/2017/05/16/charles-4.0.2-cracked
用法: 输入charles，选择本地已安装的版本，点击生成，并下载charles.jar文件, 替换本地安装目录中lib目录下的charles.jar文件即可

直接用这个地址也行

https://www.zzzmode.com/mytools/charles/

证书配置：

查看电脑代理是否开启，具体操作是Proxy→Proxy Settings，打开代理设置页面，设置端口号，然后在手机端打开WLAN，设置代理主机为电脑的IP地址，端口为刚才输入的端口就行了。

后面手机端需要打开浏览器输入chls.pro/ssl，下载证书。小米自带的没反应，用手机百度发现可以了。



###### mitmproxy是一个支持HTTP和HTTPS的抓包程序，类似Fiddler、Charles的功能，只不过它通过控制台的形式操作。

安装方式：

```
pipenv install mitmproxy
```

证书配置

运行下面命令产生CA证书，并启动mitmdump:

mitudump

不过中间报错了，原因是anaconda的问题，如果是原生python是没有问题的，由于我原生python3.8，前面的tesserocr安装需要python3.7，所以选了anaconda里的python解释器。

报错；

```
from _sqlite3 import * ImportError: DLL load failed: 找不到指定的模块。
```

原因是anaconda里的DLLs文件夹下缺少sqlite3.dll文件，去

[Sqlite的官网]: https://sqlite.org/download.html

下载。找准那个 **Precompiled Binaries for Windows** 那一项就是的了。

并**解压得到的sqlite3.dll、sqlite3.def拷贝到annconda安装路径的 dlls目录下**，即可找到sqlite3模块了。

![img](https://bbsmax.ikafan.com/static/L3Byb3h5L2h0dHBzL2ltZzIwMTguY25ibG9ncy5jb20vaS1iZXRhLzExOTc0NDgvMjAxOTExLzExOTc0NDgtMjAxOTExMjYyMDM1NTg0MjctMTQzNzU5OTc0OC5wbmc=.jpg)

![img](https://bbsmax.ikafan.com/static/L3Byb3h5L2h0dHBzL2ltZzIwMTguY25ibG9ncy5jb20vaS1iZXRhLzExOTc0NDgvMjAxOTExLzExOTc0NDgtMjAxOTExMjYyMDQwMTc0ODYtMjA2MzExMjc5OC5wbmc=.jpg)

解决完这个问题之后，运行mitmdump之后，在用户目录下的.mitmproxy目录里找到CA证书。

双击mitmproxy-ca.p12就可以导入windows端的证书了。

再往安卓手机上装证书时，有一个报错，发现证书安装不了。原因是书上写的证书文件错了。应该把mitmproxy-ca-cert.cer发到安卓手机上，而不是mitmproxy-ca-cert.pem。

##### 爬虫框架的安装

###### pyspider

带有强大的WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器。同时支持多种数据库后端、多种消息队列、另外还支持JavaScript渲染页面的爬取。pyspider支持JavaScript渲染，过程是依赖于PhantomJS，所以看样子是躲不过去安装这个了。

```
pipenv install pyspider
```

然后通过命令pyspider all测试，发现报错了

```
  File "d:\github\spider\spider-study\.venv\lib\site-packages\pyspider\run.py", line 231
    async=True, get_object=False, no_input=False):
        ^
SyntaxError: invalid syntax

```

原因是python3.5中引入了async和await。所以在python3.7中已经是关键字了。需要替换掉报错文件中的关键字。分别在run.py、fetcher>tornado_fetcher.py、 webui>app.py中查找async并替换掉就可以了。

解决完这个报错之后再次运行，发现又报错了，报错如下

```
Traceback (most recent call last):
  File "D:\anaconda3\anaconda\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "D:\anaconda3\anaconda\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "D:\GitHub\spider\spider-study\.venv\Scripts\pyspider.exe\__main__.py", line 7, in <module>
  File "d:\github\spider\spider-study\.venv\lib\site-packages\pyspider\run.py", line 754, in main
    cli()
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\core.py", line 782, in main
    rv = self.invoke(ctx)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\core.py", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\decorators.py", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\pyspider\run.py", line 497, in all
    ctx.invoke(webui, **webui_config)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\click\decorators.py", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\pyspider\run.py", line 384, in webui
    app.run(host=host, port=port)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\pyspider\webui\app.py", line 59, in run
    from .webdav import dav_app
  File "d:\github\spider\spider-study\.venv\lib\site-packages\pyspider\webui\webdav.py", line 216, in <module>
    dav_app = WsgiDAVApp(config)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\wsgidav\wsgidav_app.py", line 134, in __init__
    _check_config(config)
  File "d:\github\spider\spider-study\.venv\lib\site-packages\wsgidav\wsgidav_app.py", line 118, in _check_config
    raise ValueError("Invalid configuration:\n  - " + "\n  - ".join(errors))
ValueError: Invalid configuration:
  - Deprecated option 'domaincontroller': use 'http_authenticator.domain_controller' instead.

```

原因是因为WsgiDAV发布了版本 pre-release 3.x。

有两个办法：

1.在安装包中找到pyspider的资源包，然后找到webui文件里面的webdav.py文件打开，修改第209行即可。

把

```
'domaincontroller': NeedAuthController(app),
```

修改为

```
'http_authenticator':{
        'HTTPAuthenticator':NeedAuthController(app),
    },
```

2.

将wsgidav替换为2.4.1
•# python -m pip install wsgidav==2.4.1

在pycharm自带的命令行中运行会卡住，在cmd中运行也会卡住，后面发现需要使用管理员打开cmd运行才可以。

```
(spider-study) D:\GitHub\spider\spider-study>pyspider all
d:\github\spider\spider-study\.venv\lib\site-packages\pyspider\libs\utils.py:196: FutureWarning: timeout is not supported on your platform.
  warnings.warn("timeout is not supported on your platform.", FutureWarning)
[W 200730 23:42:04 run:413] phantomjs not found, continue running without it.
[I 200730 23:42:06 result_worker:49] result_worker starting...

```

每次都卡到这不动了。后面发现连管理员也不行同样被卡住。我想可能是因为之前启动占用端口之类的原因，把pycharm关掉，然后再打开不用管理员结果也正常了。

为什么要用pyspider啊，为啥还这么多问题啊。

###### scrapy

安装scrapy需要几个依赖库lxml、pyOpenSSL、Twisted、PyWin32

###### Scrapy-Splash

是一个Scrapy中支持JavaScript的渲染的工具。安装分为两部分，一个是Splash服务的安装，具体是通过docker

```
docer run -p 8050:8050 scrapinghub/splash
docer run -d -p 8050:8050 scrapinghub/splash
```

下面多了-d参数，代表docker容器以守护态运行，这样终端远程服务连接后，不会终止splash服务运行。

再然后用pipenv安装scrapy-splash。

###### Scrapy-Redis

是Scrapy的分布式扩展模块，可以方便地实现Scrapy分布式爬虫的搭建。

```
pipenv install scrapy-redis
```

##### 部署相关库的安装

###### docker

是一种容器技术，可以将应用打包，形成一个独立的类似APP的应用。使用docker可以让每个应用彼此相互隔离，在同一台机器上同时运行多个应用(小米的应用双开)，不过它们彼此之间共享一个操作系统。docker的优势在于，它可以在更细的粒度上进行资源管理，也比虚拟化技术更加节省资源。

安装配置在另外一篇文章里。

###### Scrapyd

用于部署和运行Scrapy项目的工具。有了它，可以将写好的scrapy项目上传到云主机并通过API来控制它的运行。

这个需要部署到linux上，等安装好linux虚拟机时再来。

###### Scrapyd-Client

在scrapy代码部署到远程Scrapyd的时候，第一步需要将代码打包为EGG文件，其次将EGG文件上传到远程主机。Scrapyd-Client主要就是做这事。

###### Scrapyd API

安装好了Scrapyd之后，可以直接请求它的API来获取当前主机的Scrapy任务运行情况，但有点繁琐。所以可以用Scrapy API代替。

```
pipenv install python-scrapyd-api
```

测试代码

```python
from scrapyd_api import ScrapydAPI
scrapyd = ScrapydAPI('http://localhost:6800')
print(scrapyd.list_projects())
```

###### Scrapyrt

scrayrt为Scrapy提供了一个调度的HTTP接口。有了它，就不需要再执行Scrapy命令而是通过请求一个HTTP接口来调度Scrapy任务了。

```
pipenv install scrapyrt
```

###### Gerapy

是一个Scrapy分布式管理模块。

```
pipenv install gerapy
```

安装过程中报错了

```shell
Installing gerapy…
Adding gerapy to Pipfile's [packages]…
Installation Succeeded
Pipfile.lock (b1ca21) out of date, updating to (c11fcc)…
Locking [dev-packages] dependencies…
Locking [packages] dependencies…
 Locking...Building requirements...
[=== ]Resolving dependencies...
Locking Failed!
[ResolutionFailure]:   File "d:/python3.8/lib/site-packages/pipenv/resolver.py", line 785, in _main
[ResolutionFailure]:       resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages)
[ResolutionFailure]:   File "d:/python3.8/lib/site-packages/pipenv/resolver.py", line 746, in resolve_packages
[ResolutionFailure]:       results, resolver = resolve(
[ResolutionFailure]:   File "d:/python3.8/lib/site-packages/pipenv/resolver.py", line 728, in resolve
[ResolutionFailure]:       return resolve_deps(
[ResolutionFailure]:   File "d:\python3.8\lib\site-packages\pipenv\utils.py", line 1378, in resolve_deps
[ResolutionFailure]:       results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(
[ResolutionFailure]:   File "d:\python3.8\lib\site-packages\pipenv\utils.py", line 1093, in actually_resolve_deps
[ResolutionFailure]:       resolver.resolve()
[ResolutionFailure]:   File "d:\python3.8\lib\site-packages\pipenv\utils.py", line 818, in resolve
[ResolutionFailure]:       raise ResolutionFailure(message=str(e))
[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.
  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.
 Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.
  Hint: try $ pipenv lock --pre if it is a pre-release dependency.
ERROR: Could not find a version that matches cryptography<3.0,==2.8,>=2.0,>=2.8,>=2.9 (from scrapy==2.2.1->-r C:\Users\48967\AppData\Local\Temp\pipenvk9tmgmohrequirements\pipenv-pou4mxz_-constraints.txt (line 12))
Tried: 0.1, 0.2, 0.2.1, 0.2.2, 0.3, 0.4, 0.5, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.6, 0.6.1, 0.7, 0.7.1, 0.7.2, 0.8, 0.8.1, 0.8.2, 0.9, 0.9.1, 0.9.2, 0.9.3, 1.0, 1.0.1, 1.0.2, 1.1, 1.1.1, 1.1.2, 1.2, 1.2.1, 1.2.2, 1.2.3, 1.3, 1.3.1, 1.3.2,
 1.3.3, 1.3.4, 1.4, 1.5, 1.5.1, 1.5.2, 1.5.3, 1.6, 1.7, 1.7.1, 1.7.2, 1.8, 1.8.1, 1.8.2, 1.9, 2.0, 2.0.1, 2.0.2, 2.0.3, 2.1, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2, 2.2.1, 2.2.2, 2.3, 2.3.1, 2.4, 2.4.1, 2.4.2, 2.5, 2.6, 2.6.1, 2.7, 2.8, 2.
8, 2.9, 2.9, 2.9.1, 2.9.1, 2.9.2, 2.9.2, 3.0, 3.0
There are incompatible versions in the resolved dependencies:
  cryptography<3.0,>=2.9 (from mitmproxy==5.2->-r C:\Users\48967\AppData\Local\Temp\pipenvk9tmgmohrequirements\pipenv-pou4mxz_-constraints.txt (line 11))
  cryptography==2.8 (from gerapy==0.9.3->-r C:\Users\48967\AppData\Local\Temp\pipenvk9tmgmohrequirements\pipenv-pou4mxz_-constraints.txt (line 14))
  cryptography>=2.0 (from scrapy==2.2.1->-r C:\Users\48967\AppData\Local\Temp\pipenvk9tmgmohrequirements\pipenv-pou4mxz_-constraints.txt (line 12))
  cryptography>=2.8 (from pyopenssl==19.1.0->-r C:\Users\48967\AppData\Local\Temp\pipenvk9tmgmohrequirements\pipenv-pou4mxz_-constraints.txt (line 24))

```

意思大概就是有四个库依赖cryptography，结果四个库依赖的版本有冲突。上面还给了一些提示操作

首先使用pipenv lock --clear清除依赖项缓存，然后再尝试原命令。

试了一下，这个命令也报同样的错误。

